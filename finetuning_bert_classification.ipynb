{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP3IEYuHe0eXEds7OTr1h6k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldor07/Essay-Evaluator-Research-Assistantship/blob/main/finetuning_bert_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMSR_WILGgd1",
        "outputId": "b8810e08-5c51-48dc-f764-6f44937c3bac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "from transformers import AdamW\n"
      ],
      "metadata": {
        "id": "_aoJ76QeEnsN"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HwsWv8uGMJ1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A2U5Y4yuMJaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load and preprocess data\n",
        "df = pd.read_csv('/content/essayData.csv')\n",
        "\n",
        "# Converting labels from comma separated string to list of strings\n",
        "df['terms'] = df['terms'].apply(lambda x: x.split(','))\n",
        "\n",
        "# Strip leading/trailing whitespaces from the labels\n",
        "df['terms'] = df['terms'].apply(lambda x: [i.strip() for i in x])\n",
        "\n",
        "# Binarize the labels\n",
        "mlb = MultiLabelBinarizer()\n",
        "df['labels'] = list(mlb.fit_transform(df['terms']))\n",
        "\n",
        "# Checking the classes we have\n",
        "print(mlb.classes_)\n",
        "\n",
        "# Split dataframe\n",
        "train_data, test_data = train_test_split(df, test_size=0.4, random_state=42)\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "test_data = test_data.reset_index(drop=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEOIakjSE4xG",
        "outputId": "50fddbce-aecc-4df1-adfd-c3597172c381"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['business' 'humanities' 'sciences' 'technology']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ-t1b7kFodh",
        "outputId": "449d3e78-f02b-4602-e159-058998e973e1"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Essay  Section                                           ABSTRACT  \\\n",
            "0        1        1  Four in five Singaporeans say that they can co...   \n",
            "1        1        1  While the terms ÃfalsehoodsÃ and Ãfake news...   \n",
            "2        1        2  The digital era is characterised by easy creat...   \n",
            "3        1        2  It is important to be critical of fake news du...   \n",
            "4        1        3  Fake news has played an enormous role during C...   \n",
            "..     ...      ...                                                ...   \n",
            "231     10       10  The average temperature of Singapore has been ...   \n",
            "232     10       11  The plants along the facade require large amou...   \n",
            "233     10       12  Currently, AI regulates the release of water b...   \n",
            "234     10       13  We would like to merge and build upon these ex...   \n",
            "235     10       14  We have analysed the severity of global water ...   \n",
            "\n",
            "                                terms        labels  \n",
            "0                        [humanities]  [0, 1, 0, 0]  \n",
            "1                        [humanities]  [0, 1, 0, 0]  \n",
            "2                        [humanities]  [0, 1, 0, 0]  \n",
            "3              [sciences, humanities]  [0, 1, 1, 0]  \n",
            "4                        [humanities]  [0, 1, 0, 0]  \n",
            "..                                ...           ...  \n",
            "231                      [technology]  [0, 0, 0, 1]  \n",
            "232              [business, sciences]  [1, 0, 1, 0]  \n",
            "233            [technology, sciences]  [0, 0, 1, 1]  \n",
            "234            [technology, sciences]  [0, 0, 1, 1]  \n",
            "235  [business, technology, sciences]  [1, 0, 1, 1]  \n",
            "\n",
            "[236 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set some parameters\n",
        "MAX_LEN = 200\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 4\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 1e-05\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', truncation=True, do_lower_case=True)\n",
        "\n",
        "# Define the class for the dataset\n",
        "class Triage(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.data.ABSTRACT[index])\n",
        "        title = \" \".join(title.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs['token_type_ids']  # Add this line\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),  # Add this line\n",
        "            'targets': torch.tensor(self.data.labels[index], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "# Create DataLoaders\n",
        "training_set = Triage(train_data, tokenizer, MAX_LEN)\n",
        "testing_set = Triage(test_data, tokenizer, MAX_LEN)\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)\n"
      ],
      "metadata": {
        "id": "te3pa8Y7Gv2f"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomBERT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomBERT, self).__init__()\n",
        "        self.l1 = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 4)  # Change the output size to 4\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "MqOZTAt6GzVI"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomBERT()\n",
        "model.to(device)\n",
        "\n",
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.BCEWithLogitsLoss()  # Change to BCEWithLogitsLoss for multi-label classification\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Function to calcuate the accuracy of the model\n",
        "def calcuate_accu(big_idx, targets):\n",
        "    n_correct = (big_idx == targets.max(dim=1)[1]).sum().item()\n",
        "    return n_correct\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    for _, data in tqdm(enumerate(training_loader, 0)):\n",
        "        ids = data['ids'].to(device, dtype=torch.long)\n",
        "        mask = data['mask'].to(device, dtype=torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "        targets = data['targets'].to(device, dtype=torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accu(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += targets.size(0)\n",
        "\n",
        "        if _ % 5000 == 0:\n",
        "            loss_step = tr_loss / nb_tr_steps\n",
        "            accu_step = (n_correct * 100) / nb_tr_examples\n",
        "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
        "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct * 100) / nb_tr_examples}')\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    epoch_accu = (n_correct * 100) / nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tdS93doHDu3",
        "outputId": "6d1987e7-7153-4847-af4c-b4cf22d8b7b8"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "1it [00:00,  5.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.689335286617279\n",
            "Training Accuracy per 5000 steps: 25.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [00:05,  3.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 0: 18.43971631205674\n",
            "Training Loss Epoch: 0.6650684873263041\n",
            "Training Accuracy Epoch: 18.43971631205674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  4.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.6136883497238159\n",
            "Training Accuracy per 5000 steps: 37.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [00:05,  3.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 1: 19.148936170212767\n",
            "Training Loss Epoch: 0.6214131977823045\n",
            "Training Accuracy Epoch: 19.148936170212767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.5295875072479248\n",
            "Training Accuracy per 5000 steps: 37.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [00:05,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 2: 24.822695035460992\n",
            "Training Loss Epoch: 0.5852986474831899\n",
            "Training Accuracy Epoch: 24.822695035460992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.5707253217697144\n",
            "Training Accuracy per 5000 steps: 12.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [00:05,  3.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 3: 31.914893617021278\n",
            "Training Loss Epoch: 0.5396412644121382\n",
            "Training Accuracy Epoch: 31.914893617021278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  4.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.4747523069381714\n",
            "Training Accuracy per 5000 steps: 50.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [00:05,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 4: 45.39007092198582\n",
            "Training Loss Epoch: 0.4996279196606742\n",
            "Training Accuracy Epoch: 45.39007092198582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.4510856866836548\n",
            "Training Accuracy per 5000 steps: 37.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [00:05,  3.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 5: 49.645390070921984\n",
            "Training Loss Epoch: 0.459328497449557\n",
            "Training Accuracy Epoch: 49.645390070921984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  4.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.38370761275291443\n",
            "Training Accuracy per 5000 steps: 75.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [00:05,  3.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 6: 53.90070921985816\n",
            "Training Loss Epoch: 0.423645771212048\n",
            "Training Accuracy Epoch: 53.90070921985816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.42064252495765686\n",
            "Training Accuracy per 5000 steps: 37.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [00:05,  3.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 7: 58.86524822695036\n",
            "Training Loss Epoch: 0.3886156032482783\n",
            "Training Accuracy Epoch: 58.86524822695036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.3764053285121918\n",
            "Training Accuracy per 5000 steps: 62.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [00:04,  3.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 8: 58.156028368794324\n",
            "Training Loss Epoch: 0.35020240975750816\n",
            "Training Accuracy Epoch: 58.156028368794324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  4.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.42847704887390137\n",
            "Training Accuracy per 5000 steps: 62.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [00:04,  3.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 9: 59.57446808510638\n",
            "Training Loss Epoch: 0.3134437981579039\n",
            "Training Accuracy Epoch: 59.57446808510638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  4.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.29217201471328735\n",
            "Training Accuracy per 5000 steps: 50.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [00:05,  3.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 10: 64.53900709219859\n",
            "Training Loss Epoch: 0.27835561003949905\n",
            "Training Accuracy Epoch: 64.53900709219859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.25941383838653564\n",
            "Training Accuracy per 5000 steps: 37.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [00:04,  3.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 11: 63.12056737588652\n",
            "Training Loss Epoch: 0.24930295762088564\n",
            "Training Accuracy Epoch: 63.12056737588652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  4.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.20411933958530426\n",
            "Training Accuracy per 5000 steps: 37.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [00:05,  3.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 12: 68.08510638297872\n",
            "Training Loss Epoch: 0.22378453529543346\n",
            "Training Accuracy Epoch: 68.08510638297872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.17091527581214905\n",
            "Training Accuracy per 5000 steps: 75.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [00:04,  3.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 13: 63.829787234042556\n",
            "Training Loss Epoch: 0.20203787088394165\n",
            "Training Accuracy Epoch: 63.829787234042556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  4.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.16904273629188538\n",
            "Training Accuracy per 5000 steps: 75.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [00:05,  3.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 14: 61.702127659574465\n",
            "Training Loss Epoch: 0.17571299895644188\n",
            "Training Accuracy Epoch: 61.702127659574465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  4.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.1507468968629837\n",
            "Training Accuracy per 5000 steps: 75.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [00:05,  3.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 15: 62.4113475177305\n",
            "Training Loss Epoch: 0.1609327851070298\n",
            "Training Accuracy Epoch: 62.4113475177305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.1545194685459137\n",
            "Training Accuracy per 5000 steps: 50.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [00:05,  3.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 16: 65.2482269503546\n",
            "Training Loss Epoch: 0.1453128270804882\n",
            "Training Accuracy Epoch: 65.2482269503546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  4.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.1423581838607788\n",
            "Training Accuracy per 5000 steps: 25.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [00:05,  3.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 17: 62.4113475177305\n",
            "Training Loss Epoch: 0.13400823457373512\n",
            "Training Accuracy Epoch: 62.4113475177305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00,  4.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.12187384814023972\n",
            "Training Accuracy per 5000 steps: 75.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [00:05,  3.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 18: 68.08510638297872\n",
            "Training Loss Epoch: 0.12503968386186493\n",
            "Training Accuracy Epoch: 68.08510638297872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.09130550175905228\n",
            "Training Accuracy per 5000 steps: 87.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "18it [00:05,  3.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 19: 68.08510638297872\n",
            "Training Loss Epoch: 0.11290984228253365\n",
            "Training Accuracy Epoch: 68.08510638297872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def valid(model, testing_loader):\n",
        "    model.eval()\n",
        "    n_correct = 0\n",
        "    tr_loss = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
        "            ids = data['ids'].to(device, dtype=torch.long)\n",
        "            mask = data['mask'].to(device, dtype=torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "            targets = data['targets'].to(device, dtype=torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids).squeeze()\n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            n_correct += calcuate_accu(big_idx, targets)\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples += targets.size(0)\n",
        "\n",
        "            if _ % 5000 == 0:\n",
        "                loss_step = tr_loss / nb_tr_steps\n",
        "                accu_step = (n_correct * 100) / nb_tr_examples\n",
        "                print(f\"Validation Loss per 5000 steps: {loss_step}\")\n",
        "                print(f\"Validation Accuracy per 5000 steps: {accu_step}\")\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    epoch_accu = (n_correct * 100) / nb_tr_examples\n",
        "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return epoch_accu\n",
        "\n",
        "acc = valid(model, testing_loader)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcPW7IzhHdzj",
        "outputId": "e1691920-9b45-48a8-a554-dc7acff7e29b"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2it [00:00,  9.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss per 5000 steps: 0.655745267868042\n",
            "Validation Accuracy per 5000 steps: 25.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "24it [00:01, 12.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Epoch: 0.5850999560207129\n",
            "Validation Accuracy Epoch: 40.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_model_file = 'pytorch_roberta_sentiment.bin'\n",
        "output_vocab_file = './'\n",
        "\n",
        "model_to_save = model\n",
        "torch.save(model_to_save, output_model_file)\n",
        "tokenizer.save_vocabulary(output_vocab_file)\n",
        "\n",
        "print('All files saved')\n",
        "print('This tutorial is completed')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PV13CXfQFXC",
        "outputId": "125278ff-b45e-4829-aa99-f6f6c57443ed"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All files saved\n",
            "This tutorial is completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zufi02vpbSKb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}